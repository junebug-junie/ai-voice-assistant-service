# services common to both development and production environments.

version: '3.8'

services:
  llm-brain:
    image: ollama/ollama
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    networks:
      - app-net

  voice-app:
    build: .
    volumes:
      - ./templates:/app/templates
      #- ./static:/app/static
    ports:
      - "8080"
    depends_on:
      llm-brain: { condition: service_started }
    restart: unless-stopped
    environment:
      - LLM_URL=http://llm-brain:11434/api/chat
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WHISPER_DEVICE=${WHISPER_DEVICE}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE}
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia
    networks:
      - app-net

volumes:
  ollama_data:

networks:
  app-net:
    driver: bridge

