services:
  llm-brain:
    image: ollama/ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ollama_data:/root/.ollama
    ports:
      - "11434:11434"
    restart: unless-stopped
    networks:
      - app-net

  voice-app:
    build: .
    volumes:
      - ./templates:/app/templates
    ports:
      - "8080"
    depends_on:
      llm-brain: { condition: service_started }
    restart: unless-stopped
    environment:
      - LLM_URL=http://llm-brain:11434/api/chat
      - LLM_MODEL=${LLM_MODEL}
      - WHISPER_MODEL_SIZE=${WHISPER_MODEL_SIZE}
      - WHISPER_DEVICE=${WHISPER_DEVICE}
      - WHISPER_COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE}
      - NVIDIA_VISIBLE_DEVICES=all
    runtime: nvidia
    networks:
      - app-net

volumes:
  ollama_data:

networks:
  app-net:
    driver: bridge
