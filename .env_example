#This is an example environment file.
#Copy this file to .env and fill in your desired settings.

--- Faster-Whisper Model Configuration ---
#Specifies the size of the Whisper model to use for transcription.
#Smaller models are faster but less accurate. Larger models are more accurate but require more VRAM.
#Recommended options: tiny.en, base.en, small.en, medium.en, distil-medium.en, large-v3
WHISPER_MODEL_SIZE=distil-medium.en

#Specifies the device to run the Whisper model on.
#Use "cuda" for NVIDIA GPUs or "cpu" for the processor.
WHISPER_DEVICE=cuda

#Specifies the computation type for the model on the GPU.
#Using float16 is a good balance of speed and precision for most modern GPUs.
#Options: float16, int8_float16, int8\
WHISPER_COMPUTE_TYPE=float16

# mistral or mixtral
LLM_MODEL=mixtral
